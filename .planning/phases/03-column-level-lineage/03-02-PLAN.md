---
phase: 03-column-level-lineage
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - agents/lineage/tools/aws_extractor.py
  - agents/lineage/tools/lineage_store.py
  - lambdas/lineage_extractor/handler.py
  - lambdas/lineage_extractor/requirements.txt
  - infra/lib/lineage-stack.ts
autonomous: true

must_haves:
  truths:
    - "System extracts query history from Redshift SVL_STATEMENTTEXT"
    - "System extracts query history from Athena via boto3 API"
    - "Extracted queries are parsed for column lineage and stored"
    - "Lineage extraction runs on a schedule via Lambda"
    - "Duplicate queries are detected and skipped via sql_hash"
  artifacts:
    - path: "agents/lineage/tools/aws_extractor.py"
      provides: "Query extraction from Redshift and Athena"
      exports: ["extract_redshift_queries", "extract_athena_queries"]
    - path: "agents/lineage/tools/lineage_store.py"
      provides: "Store parsed lineage to Supabase"
      exports: ["store_lineage_result", "upsert_lineage_node", "create_lineage_edge"]
    - path: "lambdas/lineage_extractor/handler.py"
      provides: "Scheduled Lambda for batch lineage extraction"
      exports: ["handler"]
    - path: "infra/lib/lineage-stack.ts"
      provides: "CDK stack for lineage infrastructure"
      exports: ["LineageStack"]
  key_links:
    - from: "agents/lineage/tools/aws_extractor.py"
      to: "Redshift Data API"
      via: "boto3 redshift-data client"
      pattern: "redshift_data\\.execute_statement"
    - from: "agents/lineage/tools/aws_extractor.py"
      to: "Athena API"
      via: "boto3 athena client"
      pattern: "athena\\.list_query_executions|athena\\.get_query_execution"
    - from: "lambdas/lineage_extractor/handler.py"
      to: "agents/lineage/tools/*"
      via: "batch extraction orchestration"
      pattern: "extract.*queries.*store_lineage"
---

<objective>
Implement AWS lineage extraction from Redshift and Athena query logs, with batch processing and storage to the lineage graph.

Purpose: Automatically populate the lineage graph by extracting query history from AWS data sources, parsing SQL for column dependencies, and storing the resulting lineage edges. This enables users to see data flow without manual lineage definition.

Output: AWS extraction tools, lineage storage functions, and scheduled Lambda for batch extraction.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-column-level-lineage/03-RESEARCH.md
@.planning/phases/03-column-level-lineage/03-01-SUMMARY.md
@agents/profiler/tools/connectors.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: AWS Extractor and Lineage Store Tools</name>
  <files>
    agents/lineage/tools/aws_extractor.py
    agents/lineage/tools/lineage_store.py
  </files>
  <action>
    Implement AWS query extraction and lineage storage tools:

    **tools/aws_extractor.py:**

    `extract_redshift_queries(workgroup: str, region: str = "us-east-1", since_hours: int = 24) -> str`:
    - Use boto3 redshift-data client (serverless-friendly, matches profiler pattern)
    - Query SVL_STATEMENTTEXT for recent DDL/DML:
      ```sql
      SELECT
          userid, xid, starttime,
          LISTAGG(text) WITHIN GROUP (ORDER BY sequence) as query_text
      FROM svl_statementtext
      WHERE starttime > DATEADD(hour, -{since_hours}, GETDATE())
      AND type IN ('DDL', 'QUERY')
      GROUP BY userid, xid, starttime
      ORDER BY starttime DESC
      LIMIT 1000
      ```
    - Filter out utility queries (SHOW, DESCRIBE, SET, etc.)
    - Return JSON with list of {id, query_text, executed_at, database}

    `extract_athena_queries(region: str = "us-east-1", since_hours: int = 24, max_results: int = 50) -> str`:
    - Use boto3 athena client list_query_executions + get_query_execution
    - Filter for SUCCEEDED queries within time window
    - Extract query text, database, timestamp
    - Skip queries that are just catalog operations
    - Return JSON with list of {id, query_text, database, executed_at}

    `get_glue_catalog_schema(database: str, table: str, region: str = "us-east-1") -> str`:
    - Use boto3 glue client get_table
    - Extract column names and types from StorageDescriptor
    - Return JSON schema in SQLGlot MappingSchema format
    - This provides schema context for accurate lineage extraction

    **tools/lineage_store.py:**

    `upsert_lineage_node(node_type: str, namespace: str, name: str, parent_id: str = None, data_type: str = None, metadata: str = "{}") -> str`:
    - Insert or update node in lineage_nodes table
    - Use ON CONFLICT (node_type, namespace, name, parent_id) DO UPDATE
    - Return node ID (new or existing)

    `create_lineage_edge(source_id: str, target_id: str, edge_type: str, transformation_type: str, transformation_subtype: str, description: str = None, job_id: str = None, sql_hash: str = None) -> str`:
    - Insert edge in lineage_edges table
    - Skip if duplicate (source_id, target_id, job_id combination exists)
    - Return edge ID or existing edge ID if duplicate

    `store_lineage_result(lineage_result: str, job_id: str = None) -> str`:
    - Parse SQLLineageResult JSON
    - Upsert source table nodes and column nodes
    - Upsert target table node and column nodes
    - Create edges between source and target columns with transformation info
    - Return JSON with counts: {nodes_created, edges_created, duplicates_skipped}

    `check_sql_processed(sql_hash: str) -> str`:
    - Check if SQL hash already exists in lineage_edges
    - Return JSON with {processed: bool, edge_count: int}
    - Used to skip duplicate processing

    Update agents/lineage/tools/__init__.py to export new functions.
  </action>
  <verify>
    cd agents/lineage && python -c "
from tools.aws_extractor import extract_redshift_queries, extract_athena_queries, get_glue_catalog_schema
from tools.lineage_store import upsert_lineage_node, create_lineage_edge, store_lineage_result
print('AWS extractor and lineage store tools imported successfully')"
  </verify>
  <done>
    AWS extractor tools query Redshift SVL_STATEMENTTEXT and Athena query history. Lineage store tools persist parsed lineage to Supabase with deduplication via sql_hash.
  </done>
</task>

<task type="auto">
  <name>Task 2: Lineage Extractor Lambda and CDK Stack</name>
  <files>
    lambdas/lineage_extractor/handler.py
    lambdas/lineage_extractor/requirements.txt
    infra/lib/lineage-stack.ts
  </files>
  <action>
    Create Lambda for scheduled lineage extraction and supporting infrastructure:

    **lambdas/lineage_extractor/handler.py:**

    Main handler for batch lineage extraction:
    - Parse event for extraction config:
      - source_type: "redshift" | "athena" | "all"
      - since_hours: int (default 24)
      - workgroup: str (for Redshift, from env if not provided)
      - region: str (default "us-east-1")
    - Create lineage_run record with status "running"
    - For each source:
      1. Extract queries using aws_extractor
      2. Get schema context from Glue Catalog
      3. For each query:
         - Check if sql_hash already processed
         - Parse SQL using sql_parser.parse_sql_lineage
         - Store results using lineage_store.store_lineage_result
      4. Track counts: queries_processed, edges_created, errors
    - Update lineage_run with status "completed" or "failed"
    - Return summary JSON

    Error handling:
    - Continue on individual query parse failures (log and skip)
    - Fail run only on infrastructure errors (Supabase down, etc.)
    - Store error_message in lineage_run for debugging

    Use Lambda Powertools for logging and tracing:
    ```python
    from aws_lambda_powertools import Logger, Tracer
    from aws_lambda_powertools.utilities.typing import LambdaContext

    logger = Logger()
    tracer = Tracer()

    @logger.inject_lambda_context
    @tracer.capture_lambda_handler
    def handler(event: dict, context: LambdaContext) -> dict:
        ...
    ```

    **lambdas/lineage_extractor/requirements.txt:**
    ```
    boto3>=1.35.0
    aws-lambda-powertools>=3.0.0
    sqlglot>=26.0.0
    pydantic>=2.0.0
    httpx>=0.28.0
    ```

    **infra/lib/lineage-stack.ts:**

    CDK stack for lineage infrastructure:

    1. Lambda function for lineage_extractor:
       - Runtime: Python 3.11
       - Timeout: 15 minutes (long extraction jobs)
       - Memory: 1024 MB (SQLGlot parsing needs memory)
       - Environment:
         - SUPABASE_URL, SUPABASE_KEY from Secrets Manager (use existing pattern from dq-recommender-stack)
         - REDSHIFT_WORKGROUP from environment variable
       - IAM permissions:
         - redshift-data:ExecuteStatement, redshift-data:GetStatementResult, redshift-data:DescribeStatement
         - athena:ListQueryExecutions, athena:GetQueryExecution
         - glue:GetTable, glue:GetDatabase
         - secretsmanager:GetSecretValue (for Supabase credentials)

    2. EventBridge Rule for scheduled extraction:
       - Schedule: rate(1 hour) - hourly extraction
       - Target: lineage_extractor Lambda
       - Input: { "source_type": "all", "since_hours": 2 } (overlap for safety)
       - Enabled by default

    3. API Gateway HTTP endpoint for manual trigger:
       - POST /lineage/extract -> lineage_extractor Lambda
       - Request body: { source_type, since_hours, workgroup }
       - CORS enabled for frontend

    Export Lambda ARN, API endpoint URL, and EventBridge rule ARN.
  </action>
  <verify>
    # Check Lambda handler syntax
    python -c "import ast; ast.parse(open('lambdas/lineage_extractor/handler.py').read())"

    # CDK synth
    cd infra && npx cdk synth --quiet
  </verify>
  <done>
    Lineage extractor Lambda processes query logs from Redshift/Athena on schedule. EventBridge runs hourly extraction. API endpoint enables manual trigger. CDK stack includes all IAM permissions for AWS service access.
  </done>
</task>

</tasks>

<verification>
1. AWS extractor tools import without errors
2. Lineage store tools use Supabase client correctly
3. Lambda handler has proper error handling and logging
4. CDK synth produces Lambda, EventBridge rule, and API Gateway
5. IAM permissions cover Redshift Data API, Athena, and Glue Catalog
</verification>

<success_criteria>
- Redshift extraction queries SVL_STATEMENTTEXT with proper filtering
- Athena extraction uses list_query_executions + get_query_execution pattern
- Glue Catalog schema lookup provides context for SQLGlot parsing
- Lineage storage deduplicates via sql_hash to avoid reprocessing
- Lambda runs hourly via EventBridge with 2-hour lookback window
- Manual extraction available via API endpoint
- Errors logged but don't stop batch processing
</success_criteria>

<output>
After completion, create `.planning/phases/03-column-level-lineage/03-02-SUMMARY.md`
</output>
